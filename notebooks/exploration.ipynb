{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a38c3145-08df-4c98-8ee1-aafc799e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n",
    "from functools import partial\n",
    "import string\n",
    "import bitsandbytes\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import trim_mean\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcbbaed4-3455-485f-81ff-c9e4e3c3be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../src/\")\n",
    "# from get_reasoning_feats import get_ds_saes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c72650-c568-40a1-aea5-7e2240a5a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"REDACTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "524a4797-71e6-4785-aee2-06f043b02870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()   \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d9a06e-3bd8-442c-bbf5-eea39ecbbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6056570-a450-4f52-8a46-c992280524ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.27s/it]\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9d6261-4db5-4238-ab5a-726513105c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab54636b-5cc5-4018-8544-e330ce149a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b1af05-f1e4-49cf-b97b-42cac36bd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"llama_scope_r1_distill\"\n",
    "sae_id = \"l25r_400m_slimpajama_400m_openr1_math\"\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained_with_cfg_and_sparsity(release, sae_id)\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97374fe5-16af-4809-b7e8-32852c01c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_aqua(query, reasoning=True, include_options=True):\n",
    "    COT_PROMPT = r'Please reason step by step, and put your final answer within \\boxed{}.'\n",
    "    question, options = query['question'], query['options']\n",
    "    joined_options = \"\\n\".join(options) if include_options else \"\"\n",
    "    if reasoning:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n{COT_PROMPT} [/INST] \\n<think>\\n'\n",
    "    else:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n [/INST] \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429e7732-5859-40fd-80d3-9f8c97d24448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_tqa(question, reasoning=True):\n",
    "    COT_PROMPT = r'Please reason step by step, and put your final answer within \\boxed{}.'\n",
    "    if reasoning:\n",
    "        return f'<s>[INST] {question}\\n{COT_PROMPT} [/INST] \\n<think>\\n'\n",
    "    else:\n",
    "        return f'<s>[INST] {question}\\n [/INST] \\n'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab75eee-aa57-460c-b6bb-cdcc304d6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedPromptDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, queries):\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attn_masks = tokenized_inputs['attention_mask']\n",
    "        self.queries = queries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'query': self.queries[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da0ea104-94a6-460a-be05-8c06e4df2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokenized(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    queries = [item['query'] for item in batch]\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f115fea-0c1f-4797-90ab-ccf6acc10683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cot_batch(ds, batch_size, tokenizer, model, collate_fn):\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        [format_prompt_aqua(q, reasoning=False, include_options=False) for q in ds],\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=8192\n",
    "    )\n",
    "\n",
    "    dataset = TokenizedPromptDataset(tokenized, ds)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    all_preds = []\n",
    "    all_generations = []\n",
    "    all_questions = []\n",
    "    MCQ_ANSWER_PROMPT = 'The correct answer is ('\n",
    "    \n",
    "    for batch_inputs, queries in tqdm(dataloader):\n",
    "        \n",
    "        input_ids = batch_inputs['input_ids'].to(model.device)\n",
    "        attention_mask = batch_inputs['attention_mask'].to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "        decoded = tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
    "        decoded = [\"\".join(out.split('[/INST]')[1:]) for out in decoded]\n",
    "        all_generations.extend(decoded)\n",
    "    \n",
    "        answer_prompts = [text + MCQ_ANSWER_PROMPT for text in decoded]\n",
    "        answer_inputs = tokenizer(answer_prompts, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            out = model(**answer_inputs)\n",
    "    \n",
    "        for i, query in enumerate(queries):\n",
    "            options = query['options']\n",
    "            letters = list(string.ascii_uppercase)[:len(options)]\n",
    "            valid_ids = tokenizer.convert_tokens_to_ids(letters)\n",
    "            logits = out.logits[i, -1, valid_ids]\n",
    "            pred_idx = torch.argmax(logits).item()\n",
    "            all_preds.append(letters[pred_idx])\n",
    "            all_questions.append(query['question'])\n",
    "\n",
    "    return all_questions, all_preds, all_generations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae4d37c0-8d58-43c4-8e58-09b01b0e711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [14:53<00:00, 55.83s/it]\n"
     ]
    }
   ],
   "source": [
    "questions, preds, cots = get_cot_batch(aqua_ds, 16, tokenizer, model, collate_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ad2cc2d-e644-4366-afd8-92c5d8c6aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [format_prompt_aqua(query, reasoning=False, include_options=False) for query in aqua_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3a761d13-206b-43bb-985f-b456b5a68207",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats = [24593, 30892, 30022, 27769, 14012, 6688, 3094, 22315, 31076, 4923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a819490f-8cb6-4046-bdea-03a2fcc5981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:31<00:00,  8.00it/s]\n",
      "100%|██████████| 254/254 [00:30<00:00,  8.29it/s]\n",
      "100%|██████████| 254/254 [00:28<00:00,  8.77it/s]\n",
      "100%|██████████| 254/254 [00:31<00:00,  8.07it/s]\n",
      "100%|██████████| 254/254 [00:28<00:00,  8.89it/s]\n",
      "100%|██████████| 254/254 [00:27<00:00,  9.26it/s]\n",
      "100%|██████████| 254/254 [00:29<00:00,  8.66it/s]\n",
      "100%|██████████| 254/254 [00:30<00:00,  8.29it/s]\n",
      "100%|██████████| 254/254 [00:30<00:00,  8.19it/s]\n",
      "100%|██████████| 254/254 [00:31<00:00,  7.95it/s]\n"
     ]
    }
   ],
   "source": [
    "q_feat_acts = defaultdict(float)\n",
    "\n",
    "for feat in query_feats:\n",
    "    ans = []\n",
    "    for i in tqdm(range(len(prompts))):\n",
    "        test_input = tokenizer(\n",
    "            prompts[i],\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "        ans.append(get_sae_acts(model, sae, test_input, 25)[0][feat])\n",
    "    q_feat_acts[feat] = np.mean([i.item() for i in ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "85715c57-18e1-442b-9871-11977cb930ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {24593: 0.13993660623105023,\n",
       "             30892: 0.07957254278319557,\n",
       "             30022: 0.4698125548719421,\n",
       "             27769: 0.33370821969950293,\n",
       "             14012: 0.5787202235985929,\n",
       "             6688: 0.4418561369415343,\n",
       "             3094: 0.32625941615405046,\n",
       "             22315: 0.5009210414480506,\n",
       "             31076: 0.3065049894682066,\n",
       "             4923: 0.2690728832417586})"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_feat_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "aeace9e9-2a62-440b-86b4-cc32d3a37f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_acts(model, sae, input_batch, layer, agg='mean'):\n",
    "\n",
    "    activation_dict = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activation_dict[\"hidden\"] = output\n",
    "\n",
    "    hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**input_batch)\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    hidden_states = activation_dict['hidden']\n",
    "    raw_feats = sae.encode(hidden_states)\n",
    "\n",
    "    if agg == 'mean':\n",
    "        result= raw_feats.mean(dim=1)\n",
    "    elif agg == 'last':\n",
    "        result = raw_feats[:, -1]\n",
    "    elif agg == 'none':\n",
    "        result = raw_feats\n",
    "\n",
    "    del hidden_states, raw_feats\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ace05388-c5c9-4cf8-8a89-d0196b9ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tokenizer(\n",
    "            prompts[7],\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "result = get_sae_acts(model, sae, test_input, 25, agg='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ed9eccd6-d88a-4e4a-8dbd-42a3ff80aca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<｜begin▁of▁sentence｜>',\n",
       " '<s',\n",
       " '>[',\n",
       " 'INST',\n",
       " ']',\n",
       " 'ĠA',\n",
       " 'Ġtrain',\n",
       " 'Ġrunning',\n",
       " 'Ġat',\n",
       " 'Ġa',\n",
       " 'Ġspeed',\n",
       " 'Ġof',\n",
       " 'Ġ',\n",
       " '100',\n",
       " 'Ġmiles',\n",
       " '/h',\n",
       " 'our',\n",
       " ',',\n",
       " 'Ġtakes',\n",
       " 'Ġ',\n",
       " '10',\n",
       " 'Ġhours',\n",
       " 'Ġto',\n",
       " 'Ġreach',\n",
       " 'Ġits',\n",
       " 'Ġdestination',\n",
       " '.',\n",
       " 'ĠAfter',\n",
       " 'Ġcovering',\n",
       " 'Ġquarter',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġdistance',\n",
       " ',',\n",
       " 'Ġit',\n",
       " 'Ġstarts',\n",
       " 'Ġraining',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ġtrain',\n",
       " 'Ġhas',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġslowed',\n",
       " 'Ġto',\n",
       " 'Ġspeed',\n",
       " 'Ġof',\n",
       " 'Ġ',\n",
       " '75',\n",
       " 'Ġmiles',\n",
       " '/h',\n",
       " 'our',\n",
       " '.',\n",
       " 'ĠWhat',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġtotal',\n",
       " 'Ġjourney',\n",
       " 'Ġduration',\n",
       " '?Ċ',\n",
       " 'Ġ[/',\n",
       " 'INST',\n",
       " ']',\n",
       " 'ĠĊ']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(test_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9000d98e-6c74-4640-adf0-49d91175a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.8564, 5.3388, 3.4417, 4.5819, 5.2501, 1.5725, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6891, 3.1992,\n",
      "        1.7240], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4894,\n",
      "        4.1716, 3.0497, 4.8346, 3.4235, 5.2717, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for feat in [14012, 22315]:\n",
    "    print(result[0][:,feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "460fd602-5e30-4782-9cb9-274800d8108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.8564, 5.3388, 3.4417, 4.5819, 5.2501, 1.5725, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6891, 3.1992,\n",
       "        1.7240], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][:, 14012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6800e224-ea57-4820-abb7-a25f647748dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_token_activations(activation_tensor, num_bins=20):\n",
    "    \"\"\"\n",
    "    activation_tensor: 1D tensor of shape [seq_len]\n",
    "    Returns a binned activation tensor of shape [num_bins]\n",
    "    \"\"\"\n",
    "    seq_len = activation_tensor.shape[0]\n",
    "    bin_indices = torch.linspace(0, num_bins, steps=seq_len, dtype=torch.long)\n",
    "    bin_indices = torch.clamp(bin_indices, max=num_bins - 1)\n",
    "\n",
    "    # Initialize bins\n",
    "    binned = torch.zeros(num_bins)\n",
    "    counts = torch.zeros(num_bins)\n",
    "\n",
    "    for i, activation in enumerate(activation_tensor):\n",
    "        idx = bin_indices[i].item()\n",
    "        binned[idx] += activation\n",
    "        counts[idx] += 1\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    counts = torch.clamp(counts, min=1)\n",
    "    return binned / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "f7c97d93-ef08-49d9-a00a-54cbe13838bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8298, 4.0186,\n",
       "        2.8984, 0.0000], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for acts in activations_per_prompt:  # acts: 1D tensor for one prompt\n",
    "    binned = bin_token_activations(acts, num_bins=num_bins)\n",
    "    feature_binned_list.append(binned.unsqueeze(0))  # Shape: [1, num_bins]\n",
    "\n",
    "# Stack and average across prompts\n",
    "all_binned = torch.cat(feature_binned_list, dim=0)  # Shape: [num_prompts, num_bins]\n",
    "avg_binned = all_binned.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4bc0e-1c8a-4159-a093-1fd8dcd8895b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
