{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ceb251d-a72f-44fa-be3b-9d302a2f52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 03:45:50 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 03:45:52,728\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GenerationConfig, AutoConfig, AutoTokenizer, BitsAndBytesConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import re\n",
    "import math\n",
    "from math_verify import parse, verify, LatexExtractionConfig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba71243e-84e3-419e-aee5-f940e79c2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_MAP = {\n",
    "    \"MATH-500\": {\n",
    "        \"args\": (\"HuggingFaceH4/MATH-500\", \"test\"),\n",
    "        \"question_key\": \"problem\",\n",
    "        \"answer_key\": \"answer\"\n",
    "    },\n",
    "    \"AIME2024\": {\n",
    "        \"args\": (\"HuggingFaceH4/aime_2024\", \"train\"),\n",
    "        \"question_key\": \"problem\",\n",
    "        \"answer_key\": \"answer\"\n",
    "    },\n",
    "    \"gpqa\": {\n",
    "        \"args\": (\"hendrydong/gpqa_diamond_mc\", \"test\"),\n",
    "        \"question_key\": \"problem\",\n",
    "        \"answer_key\": \"solution\"\n",
    "    },\n",
    "    \"gsm8k\": {\n",
    "        \"args\": (\"skrishna/gsm8k_only_answer\", \"test\"),\n",
    "        \"question_key\": \"text\",\n",
    "        \"answer_key\": \"label\"\n",
    "    },\n",
    "    \"openr1-math\": {\n",
    "        \"args\": (\"open-r1/OpenR1-Math-220k\", \"train\"),\n",
    "        \"question_key\": \"problem\",\n",
    "        \"answer_key\": \"answer\"\n",
    "    },\n",
    "    \"AIME2025\": {\n",
    "        \"args\": (\"yentinglin/aime_2025\", \"train\"),\n",
    "        \"question_key\": \"problem\",\n",
    "        \"answer_key\": \"answer\"\n",
    "    },\n",
    "    \"MMLU-Pro-math\": {\n",
    "        \"args\": (\"TIGER-Lab/MMLU-Pro\", \"test\"),\n",
    "        \"options_key\": \"options\",\n",
    "        \"question_key\": \"question\",\n",
    "        \"answer_key\": \"answer\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f25a54-0f6e-4d87-aaa7-53708172c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAP   = {\n",
    "    \"deepseek-qwen-1.5b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"deepseek-llama3-8b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"deepseek-qwen-14b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    \"qwq-32b\": \"Qwen/QwQ-32B\",\n",
    "    \"qwen3-8b\": \"Qwen/Qwen3-8B\",\n",
    "    \"deepseek-qwen3-8b\": \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    \"phi4-reasoning-plus\": \"microsoft/Phi-4-reasoning-plus\",\n",
    "    \"nemotron-7b\": \"nvidia/OpenMath-Nemotron-7B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70075b81-505d-4f38-9b88-a600d891a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"gsm8k\"\n",
    "model = \"deepseek-llama3-8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d882dd39-c762-4eec-9e69-e6ea441723dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 100907.32 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 84492.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name, split = DATASET_MAP[dataset][\"args\"]\n",
    "ds = load_dataset(dataset_name, split=split)\n",
    "question_key = DATASET_MAP[dataset][\"question_key\"]\n",
    "answer_key   = DATASET_MAP[dataset][\"answer_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3111a87b-7cd5-469b-8925-c83186b52ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = MODEL_MAP[model]\n",
    "max_pos = AutoConfig.from_pretrained(model_id).max_position_embeddings\n",
    "cfg = GenerationConfig.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d02ee6d-c721-437c-87be-107fcdc6ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 07:02:47 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-27 07:03:02 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 08-27 07:03:02 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 08-27 07:03:02 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 08-27 07:03:07 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-27 07:03:07 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-27 07:03:09 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-27 07:03:09 [model_runner.py:1108] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n",
      "INFO 08-27 07:03:09 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 08-27 07:05:28 [weight_utils.py:281] Time spent downloading weights for deepseek-ai/DeepSeek-R1-Distill-Llama-8B: 139.218102 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.04s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.02s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 07:05:43 [loader.py:458] Loading weights took 14.27 seconds\n",
      "INFO 08-27 07:05:43 [model_runner.py:1140] Model loading took 14.9889 GiB and 154.309636 seconds\n",
      "INFO 08-27 07:05:45 [worker.py:287] Memory profiling takes 1.23 seconds\n",
      "INFO 08-27 07:05:45 [worker.py:287] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.90) = 28.56GiB\n",
      "INFO 08-27 07:05:45 [worker.py:287] model weights take 14.99GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.20GiB; the rest of the memory reserved for KV Cache is 12.28GiB.\n",
      "INFO 08-27 07:05:45 [executor_base.py:112] # cuda blocks: 6287, # CPU blocks: 2048\n",
      "INFO 08-27 07:05:45 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 24.56x\n",
      "INFO 08-27 07:05:49 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-27 07:06:26 [model_runner.py:1592] Graph capturing finished in 37 secs, took 0.24 GiB\n",
      "INFO 08-27 07:06:26 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 42.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",       \n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    max_model_len=4096,\n",
    "    dtype=\"half\",\n",
    "    # gpu_memory_utilization=0.7,\n",
    "    # quantization=\"bitsandbytes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3c6843-4515-4559-bbe1-5b154996e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_params(n: int, budget: int, cfg) -> SamplingParams:\n",
    "    \"\"\"\n",
    "    Build SamplingParams from model config and given budget.\n",
    "    \"\"\"\n",
    "    kw = {\"n\": n, \"max_tokens\": budget}\n",
    "    if hasattr(cfg, \"temperature\") and cfg.temperature is not None:\n",
    "        kw[\"temperature\"] = cfg.temperature\n",
    "    if hasattr(cfg, \"top_k\") and cfg.top_k is not None:\n",
    "        kw[\"top_k\"] = cfg.top_k\n",
    "    if hasattr(cfg, \"top_p\") and cfg.top_p is not None:\n",
    "        kw[\"top_p\"] = cfg.top_p\n",
    "    return SamplingParams(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceea68a2-204b-4828-9907-af2d4001275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat(prompt: str, tokenizer):\n",
    "    \"\"\"\n",
    "    Wraps a user prompt in the vLLM chat template.\n",
    "    \"\"\"\n",
    "    conversations = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        conversations,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b955ceeb-e441-41df-83ca-efed2ee01987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [00:00<00:00, 6828.61it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "for ex in tqdm(ds):\n",
    "    q = ex[question_key]\n",
    "    prompt = (\n",
    "                f\"Problem: {q}\\n\\n\"\n",
    "                \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "            )\n",
    "    prompts.append(apply_chat(prompt, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94385383-30e5-4bba-9d12-b47879de9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12af7aca-3584-43fe-8e99-3ac0550e1b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLM.generate() got an unexpected keyword argument 'do_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1196\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1189\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1191\u001b[39m         warnings.warn(\n\u001b[32m   1192\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1193\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1194\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: LLM.generate() got an unexpected keyword argument 'do_sample'"
     ]
    }
   ],
   "source": [
    "llm.generate(prompts[0], sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ccf88-1c47-43e9-8960-32759bb44801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/6595 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 00:54:52 [scheduler.py:1768] Sequence group 51_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "WARNING 08-27 00:55:03 [scheduler.py:1768] Sequence group 41_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "WARNING 08-27 00:55:21 [scheduler.py:1768] Sequence group 30_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 20/6595 [02:09<7:29:46,  4.10s/it, est. speed input: 11.49 toks/s, output: 138.95 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 00:56:28 [scheduler.py:1768] Sequence group 26_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1%|▏         | 95/6595 [04:07<2:18:18,  1.28s/it, est. speed input: 28.91 toks/s, output: 469.42 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 00:58:31 [scheduler.py:1768] Sequence group 40_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3%|▎         | 165/6595 [05:13<1:21:02,  1.32it/s, est. speed input: 41.42 toks/s, output: 785.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 00:59:33 [scheduler.py:1768] Sequence group 83_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "WARNING 08-27 00:59:39 [scheduler.py:1768] Sequence group 73_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n",
      "WARNING 08-27 00:59:50 [scheduler.py:1768] Sequence group 63_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3%|▎         | 180/6595 [06:22<3:35:47,  2.02s/it, est. speed input: 37.46 toks/s, output: 684.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:00:42 [scheduler.py:1768] Sequence group 60_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3%|▎         | 215/6595 [07:28<2:53:11,  1.63s/it, est. speed input: 37.44 toks/s, output: 674.88 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:01:52 [scheduler.py:1768] Sequence group 65_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▎         | 245/6595 [08:13<2:55:39,  1.66s/it, est. speed input: 38.61 toks/s, output: 718.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:02:33 [scheduler.py:1768] Sequence group 79_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 260/6595 [08:48<3:30:33,  1.99s/it, est. speed input: 38.38 toks/s, output: 719.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:03:04 [scheduler.py:1768] Sequence group 76_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 275/6595 [09:18<2:53:22,  1.65s/it, est. speed input: 39.13 toks/s, output: 734.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:03:45 [scheduler.py:1768] Sequence group 81_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▍         | 305/6595 [09:45<1:30:59,  1.15it/s, est. speed input: 41.60 toks/s, output: 776.07 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:04:18 [scheduler.py:1768] Sequence group 82_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▍         | 310/6595 [10:19<4:25:09,  2.53s/it, est. speed input: 39.88 toks/s, output: 746.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:04:40 [scheduler.py:1768] Sequence group 91_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▍         | 320/6595 [10:28<3:11:08,  1.83s/it, est. speed input: 40.75 toks/s, output: 767.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:04:59 [scheduler.py:1768] Sequence group 84_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▌         | 360/6595 [11:37<3:10:42,  1.84s/it, est. speed input: 40.75 toks/s, output: 773.12 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:05:55 [scheduler.py:1768] Sequence group 105_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n",
      "WARNING 08-27 01:06:01 [scheduler.py:1768] Sequence group 105_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6%|▌         | 370/6595 [11:55<2:55:44,  1.69s/it, est. speed input: 41.14 toks/s, output: 787.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:06:15 [scheduler.py:1768] Sequence group 110_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6%|▌         | 390/6595 [12:09<1:23:36,  1.24it/s, est. speed input: 42.88 toks/s, output: 819.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:06:26 [scheduler.py:1768] Sequence group 110_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n",
      "WARNING 08-27 01:06:41 [scheduler.py:1768] Sequence group 101_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6%|▌         | 410/6595 [13:15<4:03:09,  2.36s/it, est. speed input: 40.73 toks/s, output: 801.46 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:07:34 [scheduler.py:1768] Sequence group 106_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 435/6595 [13:48<2:06:52,  1.24s/it, est. speed input: 41.66 toks/s, output: 802.74 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:08:12 [scheduler.py:1768] Sequence group 109_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 450/6595 [14:17<2:17:13,  1.34s/it, est. speed input: 41.32 toks/s, output: 812.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:08:35 [scheduler.py:1768] Sequence group 114_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 460/6595 [14:45<3:52:27,  2.27s/it, est. speed input: 40.74 toks/s, output: 801.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:09:04 [scheduler.py:1768] Sequence group 116_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 480/6595 [15:14<2:46:52,  1.64s/it, est. speed input: 41.24 toks/s, output: 798.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:09:49 [scheduler.py:1768] Sequence group 118_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 485/6595 [15:44<5:00:31,  2.95s/it, est. speed input: 40.47 toks/s, output: 789.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:10:01 [scheduler.py:1768] Sequence group 122_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 495/6595 [16:03<3:58:01,  2.34s/it, est. speed input: 40.58 toks/s, output: 790.50 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:10:21 [scheduler.py:1768] Sequence group 125_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 505/6595 [16:23<3:47:22,  2.24s/it, est. speed input: 40.76 toks/s, output: 791.80 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:10:42 [scheduler.py:1768] Sequence group 121_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 520/6595 [16:47<2:56:26,  1.74s/it, est. speed input: 41.08 toks/s, output: 804.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:11:05 [scheduler.py:1768] Sequence group 136_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 525/6595 [16:53<2:43:18,  1.61s/it, est. speed input: 41.31 toks/s, output: 813.55 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:11:15 [scheduler.py:1768] Sequence group 160_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 530/6595 [17:02<2:50:19,  1.68s/it, est. speed input: 41.49 toks/s, output: 818.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:11:19 [scheduler.py:1768] Sequence group 157_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n",
      "WARNING 08-27 01:11:26 [scheduler.py:1768] Sequence group 148_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n",
      "WARNING 08-27 01:11:37 [scheduler.py:1768] Sequence group 138_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 545/6595 [18:08<5:20:17,  3.18s/it, est. speed input: 40.37 toks/s, output: 790.69 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:12:25 [scheduler.py:1768] Sequence group 137_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9%|▉         | 580/6595 [19:05<2:44:29,  1.64s/it, est. speed input: 40.77 toks/s, output: 787.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:13:23 [scheduler.py:1768] Sequence group 142_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9%|▉         | 615/6595 [19:35<1:29:44,  1.11it/s, est. speed input: 42.23 toks/s, output: 810.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:13:54 [scheduler.py:1768] Sequence group 153_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|▉         | 645/6595 [20:01<1:16:56,  1.29it/s, est. speed input: 42.99 toks/s, output: 831.84 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:14:21 [scheduler.py:1768] Sequence group 159_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|▉         | 650/6595 [20:17<2:31:10,  1.53s/it, est. speed input: 42.86 toks/s, output: 829.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:14:38 [scheduler.py:1768] Sequence group 162_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 660/6595 [20:47<3:55:25,  2.38s/it, est. speed input: 42.51 toks/s, output: 819.27 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:15:08 [scheduler.py:1768] Sequence group 157_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 680/6595 [21:11<2:16:16,  1.38s/it, est. speed input: 43.02 toks/s, output: 827.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:15:30 [scheduler.py:1768] Sequence group 179_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|█         | 690/6595 [21:15<1:31:50,  1.07it/s, est. speed input: 43.37 toks/s, output: 832.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:15:36 [scheduler.py:1768] Sequence group 171_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█         | 695/6595 [21:34<2:42:43,  1.65s/it, est. speed input: 43.11 toks/s, output: 823.97 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:15:51 [scheduler.py:1768] Sequence group 161_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█         | 730/6595 [22:28<2:46:34,  1.70s/it, est. speed input: 43.77 toks/s, output: 831.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:16:45 [scheduler.py:1768] Sequence group 167_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█▏        | 745/6595 [23:10<3:13:33,  1.99s/it, est. speed input: 43.34 toks/s, output: 825.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:17:30 [scheduler.py:1768] Sequence group 169_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 760/6595 [23:49<3:27:16,  2.13s/it, est. speed input: 42.94 toks/s, output: 820.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:18:07 [scheduler.py:1768] Sequence group 169_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 780/6595 [24:07<1:55:19,  1.19s/it, est. speed input: 43.76 toks/s, output: 839.27 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:18:23 [scheduler.py:1768] Sequence group 195_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2301\n",
      "WARNING 08-27 01:18:31 [scheduler.py:1768] Sequence group 188_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2351\n",
      "WARNING 08-27 01:18:46 [scheduler.py:1768] Sequence group 179_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 805/6595 [24:53<1:54:35,  1.19s/it, est. speed input: 44.16 toks/s, output: 843.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:19:15 [scheduler.py:1768] Sequence group 183_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▏        | 820/6595 [25:18<2:26:02,  1.52s/it, est. speed input: 44.28 toks/s, output: 848.27 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:20:02 [scheduler.py:1768] Sequence group 189_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 840/6595 [26:04<2:23:50,  1.50s/it, est. speed input: 43.98 toks/s, output: 842.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:20:20 [scheduler.py:1768] Sequence group 206_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2551\n",
      "WARNING 08-27 01:20:30 [scheduler.py:1768] Sequence group 197_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 870/6595 [26:44<2:07:35,  1.34s/it, est. speed input: 44.29 toks/s, output: 846.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:21:00 [scheduler.py:1768] Sequence group 198_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 875/6595 [27:06<3:37:25,  2.28s/it, est. speed input: 43.92 toks/s, output: 843.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:21:26 [scheduler.py:1768] Sequence group 209_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2701\n",
      "WARNING 08-27 01:21:45 [scheduler.py:1768] Sequence group 201_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14%|█▎        | 905/6595 [28:23<2:00:19,  1.27s/it, est. speed input: 43.41 toks/s, output: 833.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:22:52 [scheduler.py:1768] Sequence group 203_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14%|█▍        | 935/6595 [29:12<2:05:37,  1.33s/it, est. speed input: 43.22 toks/s, output: 830.78 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:23:30 [scheduler.py:1768] Sequence group 222_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2851\n",
      "WARNING 08-27 01:23:37 [scheduler.py:1768] Sequence group 215_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14%|█▍        | 950/6595 [29:37<1:56:46,  1.24s/it, est. speed input: 43.32 toks/s, output: 832.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:23:54 [scheduler.py:1768] Sequence group 225_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14%|█▍        | 955/6595 [29:40<1:43:23,  1.10s/it, est. speed input: 43.39 toks/s, output: 836.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:24:00 [scheduler.py:1768] Sequence group 232_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3001\n",
      "WARNING 08-27 01:24:09 [scheduler.py:1768] Sequence group 223_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▍        | 965/6595 [30:01<2:19:17,  1.48s/it, est. speed input: 43.48 toks/s, output: 841.95 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:24:24 [scheduler.py:1768] Sequence group 222_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▌        | 1000/6595 [31:04<1:57:40,  1.26s/it, est. speed input: 43.94 toks/s, output: 845.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:25:21 [scheduler.py:1768] Sequence group 232_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▌        | 1005/6595 [31:14<2:16:38,  1.47s/it, est. speed input: 43.88 toks/s, output: 845.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:25:34 [scheduler.py:1768] Sequence group 231_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▌        | 1035/6595 [31:47<1:30:59,  1.02it/s, est. speed input: 44.21 toks/s, output: 848.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:26:06 [scheduler.py:1768] Sequence group 231_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▌        | 1045/6595 [31:54<1:16:14,  1.21it/s, est. speed input: 44.49 toks/s, output: 853.96 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:26:24 [scheduler.py:1768] Sequence group 234_parallel_sample_3 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▌        | 1070/6595 [32:39<2:02:56,  1.34s/it, est. speed input: 44.37 toks/s, output: 847.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:26:55 [scheduler.py:1768] Sequence group 258_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3351\n",
      "WARNING 08-27 01:27:01 [scheduler.py:1768] Sequence group 252_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3401\n",
      "WARNING 08-27 01:27:08 [scheduler.py:1768] Sequence group 258_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▋        | 1080/6595 [32:58<2:21:16,  1.54s/it, est. speed input: 44.38 toks/s, output: 849.12 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:27:14 [scheduler.py:1768] Sequence group 258_parallel_sample_1 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 1090/6595 [33:09<1:58:27,  1.29s/it, est. speed input: 44.52 toks/s, output: 850.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:27:29 [scheduler.py:1768] Sequence group 251_parallel_sample_4 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 1120/6595 [34:04<2:02:30,  1.34s/it, est. speed input: 44.47 toks/s, output: 846.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:28:22 [scheduler.py:1768] Sequence group 254_parallel_sample_0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 1135/6595 [34:35<2:31:29,  1.66s/it, est. speed input: 44.57 toks/s, output: 849.52 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-27 01:28:51 [scheduler.py:1768] Sequence group 261_parallel_sample_2 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3651\n"
     ]
    }
   ],
   "source": [
    "results = llm.generate(prompts=prompts, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93efadf0-f9cd-4d45-98ab-719991b41de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_answer(pred: str, ref: str) -> bool:\n",
    "\n",
    "    # ── patterns & threshold ─────────────────────────────────────────────────\n",
    "    BASE_N_RE    = re.compile(r\"^\\(?([0-9A-Za-z]+)\\)?_\\{(\\d+)\\}$\")\n",
    "    EXP_RE       = re.compile(r\"\\^\\{(\\d+)\\}\")\n",
    "    MAX_SAFE_EXP = 10_000\n",
    "\n",
    "    # ── normalize inputs ─────────────────────────────────────────────────────\n",
    "    if pred is None or ref is None:\n",
    "        return False\n",
    "    p = pred.strip()\n",
    "    r = ref.strip()\n",
    "\n",
    "    # ── 1) base-N literal in prediction ─────────────────────────────────────\n",
    "    m = BASE_N_RE.match(p)\n",
    "    if m:\n",
    "        return m.group(1) == r\n",
    "\n",
    "    # ── 2) base-N literal in reference ──────────────────────────────────────\n",
    "    m = BASE_N_RE.match(r)\n",
    "    if m:\n",
    "        return m.group(1) == p\n",
    "\n",
    "    # ── 3) huge-exponent guard ───────────────────────────────────────────────\n",
    "    exps = [int(e) for e in EXP_RE.findall(p)]\n",
    "    if exps and max(exps) > MAX_SAFE_EXP:\n",
    "        return p.replace(\" \", \"\") == r.replace(\" \", \"\")\n",
    "\n",
    "    # ── 4) fallback to math_verify ──────────────────────────────────────────\n",
    "    wrap = lambda s: f\"\\\\({s}\\\\)\"\n",
    "    cfg  = LatexExtractionConfig()\n",
    "    try:\n",
    "        g_node = parse(wrap(r), extraction_config=[cfg])\n",
    "        p_node = parse(wrap(p), extraction_config=[cfg])\n",
    "        return verify(g_node, p_node, float_rounding=2)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_answer(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    # Step 1: Remove everything that is not a number, letter, \".\", or \"-\"\n",
    "    # text = re.sub(r'[^0-9a-zA-Z{}\\\\.\\-]', '', text)\n",
    "    # Try extracting from 'boxed' first\n",
    "    boxed_matches = extract_boxed(text)\n",
    "    if boxed_matches:\n",
    "        extracted_answer = boxed_matches[-1][1:-1]\n",
    "        return extracted_answer\n",
    "\n",
    "    # Fallback: extract any numbers\n",
    "    numbers = re.findall(r'-?\\d+\\.\\d+|-?\\d+', text)\n",
    "    if not numbers:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        extracted_number = float(numbers[-1])\n",
    "        # Guard against infinity\n",
    "        if math.isinf(extracted_number):\n",
    "            return None\n",
    "        \n",
    "        return numbers[-1]\n",
    "    except (ValueError, OverflowError):\n",
    "        return None\n",
    "\n",
    "def extract_boxed(text):\n",
    "    pattern = re.compile(r'boxed\\{')\n",
    "    matches = []\n",
    "    stack = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        match = pattern.search(text, i)\n",
    "        if not match:\n",
    "            break\n",
    "        \n",
    "        start = match.end() - 1  # Position at the first `{`\n",
    "        stack.append(start)\n",
    "        i = start + 1\n",
    "        count = 1  # To track `{}` pairs\n",
    "        \n",
    "        while i < len(text) and stack:\n",
    "            if text[i] == '{':\n",
    "                count += 1\n",
    "            elif text[i] == '}':\n",
    "                count -= 1\n",
    "                if count == 0:  # Found a matching closing `}`\n",
    "                    start = stack.pop()\n",
    "                    matches.append(text[start:i+1])\n",
    "                    break\n",
    "            i += 1\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659e85ba-0129-4e9f-b53f-815b033952ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1319it [02:13,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "runs = {rid: [] for rid in range(10)}\n",
    "for idx, gen in tqdm(enumerate(results)):\n",
    "    gold = ds[idx][answer_key]\n",
    "    for rid, out in enumerate(gen.outputs):\n",
    "        text = out.text.strip()\n",
    "        # prediction extraction\n",
    "        pred = extract_answer(text)\n",
    "        # correctness\n",
    "        correct = False\n",
    "        try:\n",
    "            correct = verify_answer(gold, pred)\n",
    "        except:\n",
    "            pass\n",
    "        # reasoning length (entire response) in tokens\n",
    "        reasoning_length = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "        runs[rid].append({\n",
    "            \"question\":         ds[idx][question_key],\n",
    "            \"full_response\":    text,\n",
    "            \"reasoning_length\": reasoning_length,\n",
    "            \"prediction\":       pred,\n",
    "            \"gold\":             gold,\n",
    "            \"correct\":          correct\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f813b5ec-e4ae-4801-9ce1-c9510a368255",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"hint_results/gsm8k/deepseek-qwen-1.5b\", exist_ok=True)\n",
    "output_path = (\n",
    "    \"hint_results/gsm8k/deepseek-qwen-1.5b/\"\n",
    "    \"10_runs.json\"\n",
    ")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"runs\":[{\"run_id\":rid,\"records\":recs} for rid,recs in runs.items()]}, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
