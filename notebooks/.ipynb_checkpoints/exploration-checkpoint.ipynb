{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a38c3145-08df-4c98-8ee1-aafc799e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer\n",
    "from functools import partial\n",
    "import string\n",
    "import bitsandbytes\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import trim_mean\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c72650-c568-40a1-aea5-7e2240a5a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"REDACTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d9a06e-3bd8-442c-bbf5-eea39ecbbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6056570-a450-4f52-8a46-c992280524ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [02:35<00:00, 77.85s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.98s/it]\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9d6261-4db5-4238-ab5a-726513105c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab54636b-5cc5-4018-8544-e330ce149a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b1af05-f1e4-49cf-b97b-42cac36bd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"llama_scope_r1_distill\"\n",
    "sae_id = \"l25r_400m_slimpajama_400m_openr1_math\"\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained_with_cfg_and_sparsity(release, sae_id)\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97374fe5-16af-4809-b7e8-32852c01c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_aqua(question, reasoning=True, include_options=True):\n",
    "    # question, options = query['question'], query['options']\n",
    "    joined_options = \"\\n\".join(options) if include_options else \"\"\n",
    "    if reasoning:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n{COT_PROMPT} [/INST] \\n<think>\\n'\n",
    "    else:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n [/INST] \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e7732-5859-40fd-80d3-9f8c97d24448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_tqa(question, reasoning=True):\n",
    "    COT_PROMPT = r'Please reason step by step, and put your final answer within \\boxed{}.'\n",
    "    if reasoning:\n",
    "        return f'<s>[INST] {question}\\n{COT_PROMPT} [/INST] \\n<think>\\n'\n",
    "    else:\n",
    "        return f'<s>[INST] {question}\\n [/INST] \\n'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1438d6-d410-43ce-80d5-f096ede15557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cot(model, tokenizer, query):\n",
    "    \"\"\"\n",
    "    Function to get the response from a model to an MCQ - both the prediction and the reasoning (CoT)\n",
    "    \"\"\"\n",
    "\n",
    "    MCQ_ANSWER_EXTRACTION_PROMPT = 'The correct answer is ('\n",
    "\n",
    "    # Set up prompt for CoT case \n",
    "    prompt = format_prompt(query)\n",
    "\n",
    "    # Tokenize prompt\n",
    "    prompt_token_ids = tokenizer(\n",
    "        [prompt], \n",
    "        padding=True,\n",
    "        truncation=True, \n",
    "        max_length=8192,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate model response based on input\n",
    "    # No sampling, just deterministic output to analyze faithfulness\n",
    "    with torch.inference_mode():\n",
    "        out_tok_ids = model.generate(\n",
    "            input_ids=prompt_token_ids['input_ids'], \n",
    "            attention_mask=prompt_token_ids['attention_mask'],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.95\n",
    "        )[0]\n",
    "\n",
    "    # Decode output tokens into English and parse output to remove question + options\n",
    "    output = tokenizer.decode(out_tok_ids.cpu(), skip_special_tokens=False)\n",
    "    output = \"\".join(output.split('[/INST]')[1:])\n",
    "    \n",
    "    # Tokenize answer extraction prompt for prompting model again for answer post CoT\n",
    "    answer_extraction_prompt = tokenizer(\n",
    "        MCQ_ANSWER_EXTRACTION_PROMPT, \n",
    "        truncation=True, \n",
    "        return_tensors='pt')['input_ids'].to(model.device)\n",
    "\n",
    "    if out_tok_ids[-1] == tokenizer.eos_token_id:\n",
    "        out_tok_ids = out_tok_ids[:-1]\n",
    "    out_tok_ids_w_answer_extraction = torch.concat([out_tok_ids, answer_extraction_prompt[0]])\n",
    "    out = model(out_tok_ids_w_answer_extraction.unsqueeze(0))\n",
    "\n",
    "    # Getting valid options from ASCII and converting to integer representation \n",
    "    options = query['options']\n",
    "    letters = list(string.ascii_uppercase)[:len(options)]\n",
    "    valid_option_ids = [tokenizer.convert_tokens_to_ids(o) for o in letters]\n",
    "\n",
    "    pred_idx = torch.argmax(out.logits[0, -1, valid_option_ids]).cpu()\n",
    "    pred = tokenizer.convert_ids_to_tokens(valid_option_ids[pred_idx])\n",
    "\n",
    "    return pred, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7a3b38-fc60-44e7-9257-3f8cef87455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"question\": \"In the coordinate plane, points (x, 1) and (5, y) are on line k. If line k passes through the origin and has slope 1/5, then what are the values of x and y respectively?\", \"options\": [\"A)4 and 1\", \"B)1 and 5\", \"C)5 and 1\", \"D)3 and 5\", \"E)5 and 3\"], \"rationale\": \"Line k passes through the origin and has slope 1/5 means that its equation is y=1/5*x.\\nThus: (x, 1)=(5, 1) and (5, y) = (5,1) -->x=5 and y=1\\nAnswer: C\", \"correct\": \"C\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "70752b0e-8ae1-4ac0-8a58-6acf2c180ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C',\n",
       " ' \\n\\n<think>\\nAlright, let\\'s try to tackle this problem step by step. So, we have two points on line k: (x, 1) and (5, y). The line k passes through the origin and has a slope of 1/5. We need to find the values of x and y from the given options.\\n\\nFirst, let me recall the equation of a line in slope-intercept form. It\\'s usually written as y = mx + b, where m is the slope and b is the y-intercept. However, since the line passes through the origin, the y-intercept b should be zero. So, the equation simplifies to y = mx.\\n\\nGiven the slope is 1/5, the equation of line k becomes y = (1/5)x. That seems straightforward.\\n\\nNow, since both points (x, 1) and (5, y) lie on this line, they must satisfy the equation y = (1/5)x.\\n\\nLet\\'s plug in the first point (x, 1). Substituting into the equation, we get:\\n1 = (1/5)x.\\n\\nHmm, solving for x, I can multiply both sides by 5:\\n5 * 1 = x\\nSo, x = 5.\\n\\nWait, that seems too straightforward. Let me check if I did that correctly. If x is 5, then plugging back into the equation, y should be (1/5)*5 = 1, which matches the given point (5, y). So, y would be 1. So, does that mean the point is (5,1)? But looking at the options, both C and A have 5 and 1. Hmm, but let me check the other point as well.\\n\\nWait, actually, the other point is (5, y). So, let\\'s plug x = 5 into the equation to find y:\\ny = (1/5)*5 = 1. So, y is indeed 1. So, the point (5, y) is (5,1). But the first point is (x, 1), so if x is 5, then the first point is also (5,1). That seems like both points are the same, which can\\'t be right because they are two distinct points on the line.\\n\\nWait, maybe I made a mistake here. Let me read the problem again: \"points (x, 1) and (5, y) are on line k.\" So, both points are on line k, but they are different points. So, if (x,1) and (5, y) are distinct, then x must not be 5. So, perhaps I messed up the substitution.\\n\\nWait, when I plugged in (x, 1), I got x = 5. So, that suggests that (x,1) is (5,1). But then, if (5,1) is one point, and (5,y) is another, then y must also be 1, which would make both points the same, which contradicts the fact that they are two different points. So, that suggests that maybe my initial substitution was incorrect.\\n\\nWait, no, maybe I need to consider both points together. Let me think. Since both points are on the line y = (1/5)x, so for (x,1), 1 = (1/5)x, so x = 5. For (5, y), y = (1/5)*5 = 1. So, both points are (5,1). That can\\'t be right because they are supposed to be two different points. So, perhaps the line isn\\'t y = (1/5)x? Wait, no, the slope is 1/5, and it passes through the origin, so y = (1/5)x is correct.\\n\\nWait, maybe the problem is that the points are (x,1) and (5,y), but if (x,1) is on the line, then x must be 5, but then (5,1) is the same as (5,y), so y must also be 1. So, both points are (5,1). That seems contradictory because they should be two different points. So, maybe I misread the problem.\\n\\nWait, let me check the problem again: \"points (x, 1) and (5, y) are on line k.\" So, line k passes through the origin and has slope 1/5. So, it\\'s y = (1/5)x.\\n\\nSo, if (x,1) is on the line, then 1 = (1/5)x, so x = 5. So, that point is (5,1). Similarly, if (5,y) is on the line,')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cot(model, tokenizer, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c0f9c-d35e-424c-8f47-1d43704053e6",
   "metadata": {},
   "source": [
    "What are we trying to achieve here?\n",
    "\n",
    "We have a model and a pretrained SAE for that model. We want to try and find reasoning-related components through the use of the SAE. What features reliably fire for questions that require reasoning, vs \"regular\" questions. What features fire when the CoT prompt is present vs absent? When we run faithfulness experiments, is there any feature that corresponds to hint verbalization? Is there a difference in feature activations between models that have the same answer prior to CoT and those that use the CoT to get to the answer?\n",
    "\n",
    "So first, let's try and find reasoning-related SAE features. To do that, we can try to find features that consistently activate strongly when the model is given questions that require reasoning, but don't activate when given other random input (e.g John went to the market today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8907360a-096d-4e0e-bc82-a3bf0d8738b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 5.89k/5.89k [00:00<00:00, 17.3kB/s]\n",
      "Downloading data: 100%|██████████| 25.4M/25.4M [00:02<00:00, 11.0MB/s]\n",
      "Downloading data: 100%|██████████| 74.0k/74.0k [00:00<00:00, 213kB/s]\n",
      "Downloading data: 100%|██████████| 76.1k/76.1k [00:00<00:00, 249kB/s]\n",
      "Generating train split: 100%|██████████| 97467/97467 [00:00<00:00, 424546.28 examples/s]\n",
      "Generating test split: 100%|██████████| 254/254 [00:00<00:00, 88572.76 examples/s]\n",
      "Generating validation split: 100%|██████████| 254/254 [00:00<00:00, 93427.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "aqua_ds = load_dataset('aqua_rat', 'raw', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df4f50b-548f-4aeb-8262-c346fca5e058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'options', 'rationale', 'correct'],\n",
       "    num_rows: 254\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqua_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b4bdfb-6d60-4caf-a45b-5b231941a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_acts(input_batch, layer, agg='mean'):\n",
    "\n",
    "    activation_dict = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activation_dict[\"hidden\"] = output\n",
    "\n",
    "    hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**input_batch)\n",
    "\n",
    "    hook.remove()\n",
    "    \n",
    "    hidden_states = activation_dict['hidden']\n",
    "    raw_feats = sae.encode(hidden_states)\n",
    "    \n",
    "    if agg == 'mean':\n",
    "        result= raw_feats.mean(dim=1) \n",
    "    elif agg == 'last':\n",
    "        result = raw_feats[:, -1]\n",
    "\n",
    "    del hidden_states, raw_feats\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bdd3b2e-53ca-4e2c-b32a-f3041e6a9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedPromptDataset(Dataset):\n",
    "    def __init__(self, num_examples):\n",
    "        self.indices = list(range(num_examples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.indices[idx]\n",
    "\n",
    "def collate_tokenized(batch_indices, tokenized):\n",
    "    return {k: v[batch_indices] for k, v in tokenized.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5e9d2fa4-1bb3-4acd-9f7f-28b10d782465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_saes(sae, layer, prompts, model, collate_fn, batch_size=8, agg='mean'):\n",
    "    \n",
    "    dataset = IndexedPromptDataset(len(prompts))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    num_feats = sae.cfg.d_sae\n",
    "    sae_mat = torch.zeros(len(prompts), num_feats)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch_inputs in enumerate(tqdm(dataloader)):\n",
    "            batch_inputs = {k: v.to(model.device) for k, v in batch_inputs.items()}\n",
    "            batch_feats = get_sae_acts(batch_inputs, layer=layer, agg=agg)\n",
    "            start = i * batch_size\n",
    "            end = start + batch_feats.shape[0]\n",
    "            sae_mat[start:end] = batch_feats.cpu()\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "\n",
    "    return sae_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ff913f1f-5df5-46ed-b347-830f9efa3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_aqua(query, reasoning=True, include_options=True):\n",
    "    question, options = query['question'], query['options']\n",
    "    joined_options = \"\\n\".join(options) if include_options else \"\"\n",
    "    if reasoning:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n{COT_PROMPT} [/INST] \\n<think>\\n'\n",
    "    else:\n",
    "        return f'<s>[INST] {question}{joined_options}\\n [/INST] \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f0a1076-a8e7-4741-95a2-676477af12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_prompts = [format_prompt_aqua(q, reasoning=False, include_options=False) for q in aqua_ds['question']]\n",
    "aq_tokenized = tokenizer(aq_prompts, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "817d05ad-9e98-4615-a824-e79e424dcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_collate_fn = partial(collate_tokenized, tokenized=aq_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7df177e8-add6-4b4a-9791-ecc5aed161a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "means_r = get_ds_saes(sae, 25, aq_prompts, model, collate_fn=aq_collate_fn, agg='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "76a194b9-6012-42b4-a8e6-3f3bc147d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "lasts_r = get_ds_saes(sae, 25, aq_prompts, model, collate_fn=aq_collate_fn, agg='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66c63d8a-85c1-450b-9797-7af04d90f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 26.7k/26.7k [00:00<00:00, 120kB/s]\n",
      "Downloading data: 100%|██████████| 26/26 [02:43<00:00,  6.30s/files]\n",
      "Downloading data: 100%|██████████| 327M/327M [00:06<00:00, 48.6MB/s] \n",
      "Downloading data: 100%|██████████| 296M/296M [00:04<00:00, 67.7MB/s] \n",
      "Downloading data: 100%|██████████| 184M/184M [00:02<00:00, 63.1MB/s] \n",
      "Downloading data: 100%|██████████| 129M/129M [00:02<00:00, 61.4MB/s] \n",
      "Downloading data: 100%|██████████| 307M/307M [00:04<00:00, 71.5MB/s] \n",
      "Downloading data: 100%|██████████| 288M/288M [00:05<00:00, 53.5MB/s] \n",
      "Downloading data: 100%|██████████| 171M/171M [00:03<00:00, 46.9MB/s] \n",
      "Downloading data: 100%|██████████| 128M/128M [00:02<00:00, 61.1MB/s] \n",
      "Generating train split: 100%|██████████| 138384/138384 [00:49<00:00, 2776.21 examples/s]\n",
      "Generating validation split: 100%|██████████| 17944/17944 [00:06<00:00, 2832.55 examples/s]\n",
      "Generating test split: 100%|██████████| 17210/17210 [00:06<00:00, 2683.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "nr_ds = load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e9e5cc4e-d8d3-4e07-9c31-d9b0dc7ad9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prompts = [format_prompt_tqa(q, reasoning=False) for q in nr_ds[:250]['question']]\n",
    "tr_tokenized = tokenizer(tr_prompts, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "46f8a7ec-4dec-4700-973e-aed5c49d9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_collate_fn = partial(collate_tokenized, tokenized=tr_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9f00a5b1-a754-4c21-a2dd-1d3e14f9a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "means_nr = get_ds_saes(sae, 25, tr_prompts, model, collate_fn=tr_collate_fn, agg='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8971aaad-9920-4123-9da2-4b9dea6afd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "lasts_nr = get_ds_saes(sae, 25, tr_prompts, model, collate_fn=tr_collate_fn, agg='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b1f57806-187d-4b6d-9b8b-80e03f091458",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_aqua = trim_mean(means_r.detach(), proportiontocut=0.05, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a440941-1836-48cf-bca0-f4d4daa505d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trivia = trim_mean(means_nr.detach(), proportiontocut=0.05, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "80af5922-2c0f-461d-903b-7c06525d6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "percentage_increase = 100 * (mean_aqua - mean_trivia) / (mean_trivia + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b633bffe-222a-4e39-a257-c1dcf3d243f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = (mean_trivia > 0.01) & (mean_aqua > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8010cf91-319b-4b95-b5e8-b346064d0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_percentage_increase = percentage_increase[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1df3aa14-3e5d-4b18-97db-38741644fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = np.where(valid)[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ae612298-6681-4a0f-aa9f-344027dda865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 16334: Aqua mean = 0.7922, Trivia mean = 0.0183, % increase = 4219.80%\n",
      "Feature 22315: Aqua mean = 0.2375, Trivia mean = 0.0168, % increase = 1312.60%\n",
      "Feature 5853: Aqua mean = 0.1044, Trivia mean = 0.0420, % increase = 148.75%\n",
      "Feature 4312: Aqua mean = 0.1095, Trivia mean = 0.0536, % increase = 104.39%\n",
      "Feature 6066: Aqua mean = 0.1804, Trivia mean = 0.0936, % increase = 92.69%\n",
      "Feature 4966: Aqua mean = 0.2608, Trivia mean = 0.1587, % increase = 64.36%\n",
      "Feature 12284: Aqua mean = 0.2686, Trivia mean = 0.1935, % increase = 38.82%\n",
      "Feature 31801: Aqua mean = 2.9333, Trivia mean = 2.5805, % increase = 13.67%\n",
      "Feature 29275: Aqua mean = 7.6936, Trivia mean = 7.4193, % increase = 3.70%\n",
      "Feature 4520: Aqua mean = 0.7000, Trivia mean = 0.7067, % increase = -0.95%\n"
     ]
    }
   ],
   "source": [
    "ranked_order = np.argsort(-filtered_percentage_increase)\n",
    "ranked_feature_indices = valid_indices[ranked_order] \n",
    "reasoning_feats = []\n",
    "\n",
    "top_k = 10\n",
    "for i in range(top_k):\n",
    "    idx = ranked_feature_indices[i]\n",
    "    reasoning_feats.append(idx)\n",
    "    print(f\"Feature {idx}: Aqua mean = {mean_aqua[idx]:.4f}, \"\n",
    "          f\"Trivia mean = {mean_trivia[idx]:.4f}, \"\n",
    "          f\"% increase = {percentage_increase[idx]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "45afa78c-a6da-4a13-8871-195e7bedfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 16334: Active in 93.70% of AQuA prompts and 38.80% of Trivia prompts\n",
      "Feature 22315: Active in 96.85% of AQuA prompts and 34.80% of Trivia prompts\n",
      "Feature 5853: Active in 98.03% of AQuA prompts and 56.00% of Trivia prompts\n",
      "Feature 4312: Active in 96.06% of AQuA prompts and 60.40% of Trivia prompts\n",
      "Feature 6066: Active in 99.61% of AQuA prompts and 90.00% of Trivia prompts\n",
      "Feature 4966: Active in 99.21% of AQuA prompts and 97.20% of Trivia prompts\n",
      "Feature 12284: Active in 100.00% of AQuA prompts and 100.00% of Trivia prompts\n",
      "Feature 31801: Active in 100.00% of AQuA prompts and 100.00% of Trivia prompts\n",
      "Feature 29275: Active in 100.00% of AQuA prompts and 100.00% of Trivia prompts\n",
      "Feature 4520: Active in 99.61% of AQuA prompts and 99.60% of Trivia prompts\n"
     ]
    }
   ],
   "source": [
    "for feat in reasoning_feats:\n",
    "    print(f\"Feature {feat}: Active in {100 * (means_r[:,feat] > 0).sum() / len(aq_prompts):.2f}% of AQuA prompts and {100 * (means_nr[:,feat] > 0).sum() / len(tr_prompts):.2f}% of Trivia prompts\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e89202-0bc4-4c6f-9ca4-d4602851ccce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
